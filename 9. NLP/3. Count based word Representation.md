# 자연어 처리 



## 3. 카운트 기반의 단어표현

### [1] 다양한 단어의 표현

#### (1) 단어의 표현 방법

- 국소 표현(=이산 표현 : Discrete Representation)

  - 해당 단어 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법

- 분산 표현(=연속 표현 : Continuous Representation)

  - 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법

    

#### (2) 카운트 기반 방법

<img src= https://wikidocs.net/images/page/31767/wordrepresentation.PNG>



### [2] Bag of Words(BoW)

- 단어의 등장 순서를 고려하지 않은 빈도수 기반의 단어표현 방법

#### (1) Bag of Words란?

- 단어들의 순서를 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. 

- BoW를 만드는 과정

  - 각 단어에 고유한 정수 인덱스를 부여합니다.
  - 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.

  ````python
  from konlpy.tag import Okt
  import re
  okt = Okt()
  
  # 정규표현식을 통해 온점을 제거하는 정제 작업
  doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
  doc2 = "소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
  doc3 = doc1+''+doc2
  token = re.sub("(\.)","","정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.")
  
  # OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣음
  token = okt.morphs(token)
  
  word2index = {}
  bow = []
  ````

  ````python
  for voca in token:
      # token을 읽으면서, word2index에 없는 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다. 
      if voca not in word2index.keys():
          word2index[voca] = len(word2index)
          # print(word2index[voca])
          # print(word2index)
          # BoW 전체에 전부 기본값 1을 넣습니다.
          bow.insert(len(word2index)-1,1)
          
      else:
          # 재등장하는 단어의 인덱스
          index = word2index.get(voca)
          # print(index) # 결과 1, 4
          
          # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한합니다.
          bow[index] = bow[index] + 1
          # print(bow) # [1, 2, 1, 1, 1, 1, 1], [1, 2, 1, 1, 2, 1, 1, 1]
          # 인덱스 1번째와, 4번째에 횟수를 추가해주는 것 
          
  print(word2index)
  ````

  ````python
  # 결과
  {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}
  ````

  ````python
  # 단어에 대한 카운트
  bow
  ````

  ````python
  # 결과
  [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
  ````

  

#### (2) CountVectorizer 클래스로 BoW 만들기

````python
# 영어
from sklearn.feature_extraction.text import CountVectorizer
corpus = ['yow konw I want your love. because I love you.']
vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print(vector.fit_transform(corpus).toarray())

# 각 단어의 인덱스가 어떻게 부여되어 있는지를 출력
print(vector.vocabulary_)
````

````python
# 결과
[[1 1 2 1 1 1 1]]
{'yow': 6, 'konw': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0, 'you': 4}
````

````python
# 한국어
from sklearn.feature_extraction.text import CountVectorizer
corpus = ['정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'] # 공백을 기준으로 토큰하다보니 제대로 표현되지 않음
# (물가상승률과, 물가상승률은)이 다른 단어로 인식이 됨
vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print(vector.fit_transform(corpus).toarray())

# 각 단어의 인덱스가 어떻게 부여되어 있는지를 출력
print(vector.vocabulary_)
````

````python
# 결과
[[1 1 1 1 1 1 1]]
{'정부가': 6, '발표하는': 4, '물가상승률과': 2, '소비자가': 5, '느끼는': 0, '물가상승률은': 3, '다르다': 1}
````



#### (3) 불용어를 제거한 BoW 만들기

````python
# 사용자가 직접 정의한 불용어 사용

from sklearn.feature_extraction.text import CountVectorizer

text = ["Family is not an important thing. It's everything."]
vect = CountVectorizer(stop_words=["the", "a", "an", "is", "not"])
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
````

````python
# 결과
[[1 1 1 1 1]]
{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}
````

````python
# CountVectorizer에서 제공하는 자체 불용어 사용

from sklearn.feature_extraction.text import CountVectorizer

text = ["Family is not an important thing. It's everything"]
vect = CountVectorizer(stop_words = "english")
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
````

````python
# 결과
[[1 1 1]]
{'family': 0, 'important': 1, 'thing': 2}
````

````python
# NLTK에서 지원하는 불용어 사용

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

text = ["Family is not an important thing. It's everything."]
sw = stopwords.words("english")
vect = CountVectorizer(stop_words = sw)
print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
````

````python
# 결과
[[1 1 1 1]]
{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}
````



### [3] 문서 단어 행렬

- 행렬은 DTM
- 열행은 TDM

#### (1) 문서 단어 행렬의 표기법(Document-Term Matrix, DTM)



#### (2) 문서 단어 행렬의 한계

- 희소 표현(Sparse representation)
  - 원-핫 벡터나 단어 집합의 크기가 벡터의 차원이 되고 대부분의 값이 0이 되는 벡터입니다. 원-핫 벡터는 공간적 낭비와 계산 리소스를 증가시킬 수 있다는 점에서 단점을 가집니다. DTM 또한 이와 마찬가지입니다. 
- 단순 빈도 수 기반 접근



#### (3) TDM(Term Documents Matirx)

- 문서별로 단어의 빈도(term frequency)를 정리한 표
- 장점
  - 비정형 데이터인 텍스트를 표 형태로 정형화
  - 정형 데이터의 다양한 통계 기법 적용 가능
- 단점
  - 처리가 단순하나 어순과 맥락을 무시하는 것이 단점
- 희소 행렬
- **실습**

````python
# 데이터 다운로드
import wget

wget.download('https://raw.githubusercontent.com/euphoris/datasets/master/imdb.xlsx')
````

````python
# 데이터 열기
import pandas as pd

df = pd.read_excel('imdb.xlsx', index_col=0)
df.head()
````

*# 결과*

<img width="376" alt="스크린샷 2022-03-18 17 57 22" src="https://user-images.githubusercontent.com/96100946/158972020-6dbe2098-ba4b-4d78-89d9-260855c8fe98.png">

````python
df.shape
````

````python
# 결과
(748, 2)
````

````python
# TDM 만들기
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features=500, stop_words='english') # 불용어 처리

tdm = cv.fit_transform(df['review'])
tdm
````

````python
# 결과
<748x500 sparse matrix of type '<class 'numpy.int64'>'
	with 3434 stored elements in Compressed Sparse Row format>
````

````python
# 단어 목록
cv.get_feature_names()[:10]
````

````python
# 결과
['10',
 '20',
 '90',
 'absolutely',
 'acted',
 'acting',
 'action',
 'actor',
 'actors',
 'actress']
````

````python
len(cv.get_feature_names())
````

````python
# 결과
500
````

````python
# 단어별 총 빈도
tdm.sum(axis=0) 
````

````python
# 결과
matrix([[ 29,   3,   6,   9,   3,  43,   7,  10,  19,   3,   3,  10,   3,
           4,   3,   4,   9,   3,   3,   3,   6,   3,   4,   3,  13,   4,
 																			... ... ...
          12,  21,   3,   3,  12,   5,   3,  43,   5,   4,   3,   5,   4,
          15,   3,   4,   3,   4,   9,   4,   4,   3,   8,   3,   4,   3,
           9,   4,   4,   6,   8,   4,   4,   3,  13,   4,   6,   3,   7,
          14,   7,  17,   3,   9,  22,  23,   8,   3,   3,   3,   3,   5,
          18,   3,   5,  20,   5,   6,  11,  11,  13,   3,   3,   3,  13,
           7,   3,   5,  14,   3,   4]])
````

````python
tdm.sum(axis=1)  # 문서별 총 단어 수
````

````python
# 결과
matrix([[  5],
        [  5],
        [ 12],
        [  3],
        [  6],
        [  7],
        [  2],
        [  7],
        [  2],
        [  2],
... ... 
        [  1],
        [  4],
        [  6],
        [  1],
        [  1],
        [  5]])
````

````python
word_count = pd.DataFrame({
    '단어': cv.get_feature_names(),
    '빈도': tdm.sum(axis=0).flat # 펼쳐서 
})
````

````python
word_count
````

<img width="145" alt="스크린샷 2022-03-18 18 18 33" src="https://user-images.githubusercontent.com/96100946/158975433-9de0a65f-d0e6-4b10-8088-80f85bf401ec.png">

````python
````

````python
````













#### (4) TDM 실습-단어 구름

- wordcloud 설치

````python
# 아나콘다를 이용할 경우
! conda install -y -c conda-forge wordcloud

# 맥 또는 리눅스에서는 pip 명령어로도 설치
pip install wordcloud
````



- 데이터 불러오기

````python
# 이전에 만들어 놓은 word_count를 불러온다
import pandas as pd

word_count = pd.read_csv('word_count.csv', index_col=0)
word_count.head()
````

````python
# 결과
	단어	빈도
0	10	29
1	20	3
2	90	6
3	absolutely	9
4	acted	3
````

- 단어 구름

````python
from wordcloud import WordCloud

#font_path: 글꼴의 경로
#max_words: 워드클라우드를 그릴 단어의 개수
#background_color: 배경색 설정
#width : 가로크기(픽셀 단위)
#height: 세로크기(픽셀 단위)

wc = WordCloud(background_color = 'white', max_words=100, width=400, height=300)

word_count.set_index('단어')
````

````python
# 결과
	빈도
단어	
10	29
20	3
90	6
absolutely	9
acted	3
...	...
wrong	3
year	5
years	14
yes	3
young	4
500 rows × 1 columns
````

````python
word_count.set_index('단어')['빈도']
````

``

````python
# 결과
단어
10            29
20             3
90             6
absolutely     9
acted          3
              ..
wrong          3
year           5
years         14
yes            3
young          4
Name: 빈도, Length: 500, dtype: int64
````



#### (5) 희소 행렬 압축 방식



#### (6) TF-IDF 개념

- TF : 단어의 등장 빈도
- IDF : 특정 단어가 등장한 문서의 빈도의 역수
- 문서빈도(df) : 각 단어가 등장한 (tf > 0) 문서의 수 (자주 나올수록 df 상승, idf는 하락)
- 역문서빈도(idf) : 총 문서 수를 df로 나눈 값 (문서 간의 차이가 중요한 상황에서는 idf가 높은 단어가 좋은 단어)

- 단어 빈도와 역문서빈도를 곱한 값
- 상대적으로 적은 문서에 나오면서 특정 문서에 자주나오는 단어
- TDM에 가중치를 주는 대표적인 방법
- 다른 방법들도 가능, 분석 목적에 맞게 사용

- **실습**

````python
import pandas as pd

df = pd.read_excel('imdb.xlsx', index_col=0)
df.head()
````

*# 결과*

(이미지 붙여넣기)

````python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=500, stop_words='english')

tdm = tfidf.fit_transform(df['review'])
````

````python
word_count = pd.DataFrame({
    '단어' : tfidf.get_feature_names(),
    'tf-idf' : tdm.sum(axis=0).flat
})

word_count.sort_values('tf-idf', ascending=False)
````

````python
````

````python
````

````python
````

```python

```



