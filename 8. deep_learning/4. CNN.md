# 4-5. 딥러닝-실습(CNN)

## [2] CNN(Covolution Neural Network)

### 1.  합성곱 신경망(CNN) 소개

- 분류 대상이 이미지에서 고정된 위치에 있는 경우

- 분류 대상이 이미지의 어디에 있을지 모르는 경우 

  - 일반 Dense Layer에서 image Classificaiton을 사용하지 않음

    

- Stride 개요

  - strid는 입력 데이터에 Conv Filter를 적용할 때 Sliding Window가 이동하는 간격을 읨
  - 기본은 1이지만, 2를(2 pixel 단위로 Sliding window 이동) 적용하여 입력 feature map 대비 출력 feature map의 크기를 대략 절반으로 줄임.
  - strid를 키우면 공간적인 feature 특성을 손실할 가능성이 높아지지만, 이것이 중요 feature들의 손실을 반드시 의미하지는 않음. 오히려 불필요한 특성을 제거하는 효과를 가져올 수 있음. Convoiution 연산 속도를 향상 시킴.

- Padding 개요

  - Filter를 적용하여 Conv 연산 수행 시 출력 Feature Map이 입력 Feature Map 대비 계속적으로 작아지는 것을 막기 위해 적용

  - Filter 적용 전 보존하려는 Feature map 크기에 맞게 입력 Feature Map의 좌우 끝과 상하 끝에 각각 열과 행을 추가한 뒤, 0 값을 채워, 입력 Feature Map 사이즈를 증가 시킴.

    ...

- Padding 특징

- Pooling 특징

  

### 2. 합성곱 신경망(CNN) 구현

#### (1) CIFAR10 Dataset 생성

````python
# train images : 32x32(RGB 3채널, 4차원 - data개수 포함)

from tensorflow.keras.datasets import cifar10

(train_images, train_labels),(test_images, test_labels) = cifar10.load_data()
print("train dataset shape:", train_images.shape, train_labels.shape)
print("test dataset shape:", test_images.shape, train_labels.shape)
````

````python
# 결과
train dataset shape: (50000, 32, 32, 3) (50000, 1)
test dataset shape: (10000, 32, 32, 3) (50000, 1)
````



```python
train_images[0, :, :, :], train_labels[0, :]
```

````python
# 결과
#  첫번째 이미지에 대한 내용
(array([[[ 59,  62,  63],
         [ 43,  46,  45],
         [ 50,  48,  43],
         ...,
         [158, 132, 108],
         [152, 125, 102],
         [148, 124, 103]],
 
        [[ 16,  20,  20],
         [  0,   0,   0],
         [ 18,   8,   0],
         ...,
         [123,  88,  55],
         [119,  83,  50],
         [122,  87,  57]],
 
        [[ 25,  24,  21],
         [ 16,   7,   0],
         [ 49,  27,   8],
         ...,
         [118,  84,  50],
         [120,  84,  50],
         [109,  73,  42]],
 
        ...,
 
        [[208, 170,  96],
         [201, 153,  34],
         [198, 161,  26],
         ...,
         [160, 133,  70],
         [ 56,  31,   7],
         [ 53,  34,  20]],
 
        [[180, 139,  96],
         [173, 123,  42],
         [186, 144,  30],
         ...,
         [184, 148,  94],
         [ 97,  62,  34],
         [ 83,  53,  34]],
 
        [[177, 144, 116],
         [168, 129,  94],
         [179, 142,  87],
         ...,
         [216, 184, 140],
         [151, 118,  84],
         [123,  92,  72]]], dtype=uint8),
 array([6], dtype=uint8)) # 6번 개구리
````



````python
# labels 이름 지정
NAMES = np.array(['airplane','automobile','bird','cat','deer','dog','frog','hores','ship','truck'])
print(train_labels[:10])
````

````python
# 결과
[[6]
 [9]
 [9]
 [4]
 [1]
 [1]
 [2]
 [7]
 [8]
 [3]]
````



### (2) CIFAR10 시각화

````python
# 이미지 크기 : 32x32이며 RGB채널

import matplotlib.pyplot as plt
import cv2

plt.imshow(train_images[0])
plt.title(train_labels[0].squeeze()) # squeeze 리스트 없애주기
````



````python
# 내코드 (사이즈 조절 필요)
for i in range(2):
    for j in range(8):
        index = 8 * i + j
        plt.subplot(2, 8, index+1)
        plt.imshow(train_images[index])
        plt.title(NAMES[train_labels[index].squeeze()])
````

`이미지 첨부하기`



````python
def show_images(images, labels, ncols=8):
    figure, axs = plt.subplots(figsize=(22,6),nrows=1, ncols=ncols)
    for i in range(ncols):
        axs[i].imshow(images[i])
        axs[i].set_title(NAMES[labels[i].squeeze()])
        
show_images(train_images[:8], train_labels[:8], ncols=8)
show_images(train_images[8:16], train_labels[8:16], ncols=8)
````



### (3) Data preprocessing

- image array의 0 ~ 255 값을 0 ~ 1 사이의 값으로 변환 : 정수값 pixel값을 255.0으로 나눔
- Label array -> 원-핫 인코딩으로 변환 (sqarse catrgorical crossentropy 사용 예정)
- image array, label array -> float32형으로 변환

````python
(train_images, train_labels),(test_images, test_labels) = cifar10.load_data()

# 0 ~ 1 사이의 값으로 변환
def get_preprocessed_data(images, labels):
    images = np.array(images/255.0, dtype=np.float32)
    labels = np.array(labels, dtype=np.float32)
    
    return images, labels

train_images, train_labels = get_preprocessed_data(train_images, train_labels)
test_images, test_labels = get_preprocessed_data(test_images, test_labels)
````



```python
train_images[0, :, :, :]
```

````python
# 결과
array([[[0.23137255, 0.24313726, 0.24705882],
        [0.16862746, 0.18039216, 0.1764706 ],
        [0.19607843, 0.1882353 , 0.16862746],
        ...,
        [0.61960787, 0.5176471 , 0.42352942],
        [0.59607846, 0.49019608, 0.4       ],
        [0.5803922 , 0.4862745 , 0.40392157]],

       [[0.0627451 , 0.07843138, 0.07843138],
        [0.        , 0.        , 0.        ],
        [0.07058824, 0.03137255, 0.        ],
        ...,
        [0.48235294, 0.34509805, 0.21568628],
        [0.46666667, 0.3254902 , 0.19607843],
        [0.47843137, 0.34117648, 0.22352941]],

       [[0.09803922, 0.09411765, 0.08235294],
        [0.0627451 , 0.02745098, 0.        ],
        [0.19215687, 0.10588235, 0.03137255],
        ...,
        [0.4627451 , 0.32941177, 0.19607843],
        [0.47058824, 0.32941177, 0.19607843],
        [0.42745098, 0.28627452, 0.16470589]],

       ...,

       [[0.8156863 , 0.6666667 , 0.3764706 ],
        [0.7882353 , 0.6       , 0.13333334],
        [0.7764706 , 0.6313726 , 0.10196079],
        ...,
        [0.627451  , 0.52156866, 0.27450982],
        [0.21960784, 0.12156863, 0.02745098],
        [0.20784314, 0.13333334, 0.07843138]],

       [[0.7058824 , 0.54509807, 0.3764706 ],
        [0.6784314 , 0.48235294, 0.16470589],
        [0.7294118 , 0.5647059 , 0.11764706],
        ...,
        [0.72156864, 0.5803922 , 0.36862746],
        [0.38039216, 0.24313726, 0.13333334],
        [0.3254902 , 0.20784314, 0.13333334]],

       [[0.69411767, 0.5647059 , 0.45490196],
        [0.65882355, 0.5058824 , 0.36862746],
        [0.7019608 , 0.5568628 , 0.34117648],
        ...,
        [0.84705883, 0.72156864, 0.54901963],
        [0.5921569 , 0.4627451 , 0.32941177],
        [0.48235294, 0.36078432, 0.28235295]]], dtype=float32)
````



````python
# keras는 CNN 2d 모델
print(train_images.shape, train_labels.shape)
````

````python
# 결과
(50000, 32, 32, 3) (50000, 1)
````



````python
train_labels
````

```python
# 결과
array([[6.],
       [9.],
       [9.],
       ...,
       [9.],
       [1.],
       [1.]], dtype=float32)
```



```python
# 리스트 없애기
train_labels = train_labels.squeeze()
test_labels = test_labels.squeeze()
```



### (4) Custom Model 생성

- 전통적인 CNN 네트워크(convolution Relu, convolution Relu poop)

````python
IMAGE_SIZE = 32

# 필요한 라이브러리 import
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Activation, MaxPooling2D, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
````

````python
input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))

# 3 by 3 크기의 필터를 32개 (필터: 출력 공간의 차원(깊이를 결정))
# padding = 'same' 옵션 -> padding 면적을 알아서 계산, output값 동일

# 첫번째 convolution Relu
x = Conv2D(filters=32, kernel_size=(3, 3), padding ='same', activation='relu')(input_tensor)
x = Conv2D(filters=32, kernel_size=(3, 3), padding = 'same', activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
````

````python
# 두번째 convolution Relu
x = Conv2D(filters=64, kernel_size=(3, 3), padding ='same', activation='relu')(x)
x = Conv2D(filters=64, kernel_size=(3, 3), padding = 'same')(x)
x = Activation('relu')(x)
x = MaxPooling2D(pool_size=2)(x)
````

````python
# 세번째 convolution Relu
x = Conv2D(filters=128, kernel_size=(3, 3), padding ='same', activation='relu')(x)
x = Conv2D(filters=128, kernel_size=(3, 3), padding ='same', activation='relu')(x)
x = MaxPooling2D(pool_size=2)(x)
````

```python
x = Flatten(name='flatten')(x)
x = Dropout(rate=0.5)(x)
x = Dense(300, activation='relu', name='fc1')(x)
x = Dropout(rate=0.3)(x)
output = Dense(10, activation='softmax', name='output')(x)
```

```python
model = Model(inputs=input_tensor, outputs=output)
model.summary()
```

````python
# 결과
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 32, 32, 32)        896       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
activation (Activation)      (None, 16, 16, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 2048)              0         
_________________________________________________________________
dropout (Dropout)            (None, 2048)              0         
_________________________________________________________________
fc1 (Dense)                  (None, 300)               614700    
_________________________________________________________________
dropout_1 (Dropout)          (None, 300)               0         
_________________________________________________________________
output (Dense)               (None, 10)                3010      
=================================================================
Total params: 904,718
Trainable params: 904,718
Non-trainable params: 0
_________________________________________________________________
````

````python
model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
````



### (5) Model 학습 수행 및 테스트 데이터로 평가

````python
history = model.fit(x = train_images, y = train_labels, batch_size=64, epochs=30, validation_split=0.15)
````

````python
Epoch 1/30
WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28bdfa040> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unsupported operand type(s) for -: 'NoneType' and 'int'
...
665/665 [==============================] - 43s 65ms/step - loss: 1.9410 - accuracy: 0.2758 - val_loss: 2.3415 - val_accuracy: 0.1115
Epoch 2/30
665/665 [==============================] - 47s 70ms/step - loss: 1.5334 - accuracy: 0.4559 - val_loss: 2.3938 - val_accuracy: 0.1167
Epoch 3/30
665/665 [==============================] - 45s 68ms/step - loss: 1.5274 - accuracy: 0.4777 - val_loss: 2.3569 - val_accuracy: 0.1073
Epoch 4/30
665/665 [==============================] - 48s 73ms/step - loss: 1.6460 - accuracy: 0.4775 - val_loss: 2.3047 - val_accuracy: 0.1077
Epoch 5/30
665/665 [==============================] - 53s 80ms/step - loss: 1.8871 - accuracy: 0.4425 - val_loss: 2.3066 - val_accuracy: 0.1029
Epoch 6/30
665/665 [==============================] - 52s 78ms/step - loss: 2.3049 - accuracy: 0.0993 - val_loss: 2.3030 - val_accuracy: 0.0972
Epoch 7/30
665/665 [==============================] - 49s 74ms/step - loss: 2.3028 - accuracy: 0.0961 - val_loss: 2.3028 - val_accuracy: 0.0972
Epoch 8/30
665/665 [==============================] - 52s 79ms/step - loss: 2.3026 - accuracy: 0.1006 - val_loss: 2.3027 - val_accuracy: 0.0984
Epoch 9/30
665/665 [==============================] - 53s 79ms/step - loss: 2.3027 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.0984
Epoch 10/30
665/665 [==============================] - 49s 74ms/step - loss: 2.3027 - accuracy: 0.1026 - val_loss: 2.3029 - val_accuracy: 0.1000
Epoch 11/30
665/665 [==============================] - 50s 75ms/step - loss: 2.3026 - accuracy: 0.0983 - val_loss: 2.3027 - val_accuracy: 0.0953
Epoch 12/30
665/665 [==============================] - 49s 74ms/step - loss: 2.3027 - accuracy: 0.0994 - val_loss: 2.3028 - val_accuracy: 0.1000
Epoch 13/30
665/665 [==============================] - 48s 72ms/step - loss: 2.3027 - accuracy: 0.0975 - val_loss: 2.3027 - val_accuracy: 0.1000
Epoch 14/30
665/665 [==============================] - 52s 78ms/step - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3027 - val_accuracy: 0.0984
Epoch 15/30
665/665 [==============================] - 52s 79ms/step - loss: 2.3027 - accuracy: 0.1011 - val_loss: 2.3028 - val_accuracy: 0.0972
Epoch 16/30
665/665 [==============================] - 52s 78ms/step - loss: 2.3027 - accuracy: 0.1004 - val_loss: 2.3029 - val_accuracy: 0.0972
Epoch 17/30
665/665 [==============================] - 53s 80ms/step - loss: 2.3302 - accuracy: 0.1005 - val_loss: 2.3029 - val_accuracy: 0.0953
Epoch 18/30
665/665 [==============================] - 52s 78ms/step - loss: 2.3027 - accuracy: 0.1010 - val_loss: 2.3028 - val_accuracy: 0.0984
Epoch 19/30
665/665 [==============================] - 49s 74ms/step - loss: 2.3027 - accuracy: 0.1017 - val_loss: 2.3028 - val_accuracy: 0.0972
Epoch 20/30
665/665 [==============================] - 51s 76ms/step - loss: 2.3027 - accuracy: 0.1008 - val_loss: 2.3028 - val_accuracy: 0.0975
Epoch 21/30
665/665 [==============================] - 50s 75ms/step - loss: 2.3027 - accuracy: 0.0995 - val_loss: 2.3028 - val_accuracy: 0.1000
Epoch 22/30
665/665 [==============================] - 51s 77ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.0984
Epoch 23/30
665/665 [==============================] - 50s 76ms/step - loss: 2.3027 - accuracy: 0.0961 - val_loss: 2.3030 - val_accuracy: 0.0972
Epoch 24/30
665/665 [==============================] - 52s 79ms/step - loss: 2.3027 - accuracy: 0.1006 - val_loss: 2.3028 - val_accuracy: 0.0953
Epoch 25/30
665/665 [==============================] - 52s 77ms/step - loss: 2.3027 - accuracy: 0.0991 - val_loss: 2.3029 - val_accuracy: 0.0953
Epoch 26/30
665/665 [==============================] - 50s 75ms/step - loss: 2.3026 - accuracy: 0.1023 - val_loss: 2.3027 - val_accuracy: 0.0972
Epoch 27/30
665/665 [==============================] - 40s 60ms/step - loss: 2.3027 - accuracy: 0.1016 - val_loss: 2.3028 - val_accuracy: 0.0953
Epoch 28/30
665/665 [==============================] - 30s 45ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.1000
Epoch 29/30
665/665 [==============================] - 30s 44ms/step - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3029 - val_accuracy: 0.0972
Epoch 30/30
665/665 [==============================] - 31s 47ms/step - loss: 2.3027 - accuracy: 0.0992 - val_loss: 2.3028 - val_accuracy: 0.0953
````



```python
import matplotlib.pyplot as plt

def show_history(history):
    plt.figure(figsize=(6, 6))
    plt.yticks(np.arange(0, 1, 0.05))
    plt.plot(history.history['accuracy'], label='train')
    plt.plot(history.history['val_accuracy'], label='valid')
    plt.legend()
    
show_history(history)

model.evaluate(test_images, test_labels)
```

`이미지 첨부하기`



### (6) model.predict()를 통해 이미지 분류 예측

````python
# error 발생함 
# 현재는 3차원, 4차원으로 늘려줘야함.
preds = model.predict(test_images[0])
````

````python
# 결과 
...
ValueError: Input 0 is incompatible with layer model: expected shape=(None, 32, 32, 3), found shape=(32, 32, 3)
````



````python
preds = model.predict(np.expand_dims(test_images[0],axis=0))
print('에측 결과 shape:', preds.shape)
print('에측 결과:', preds)
preds.argmax()
````











